<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Xuecheng Wu's Homepage</title>
  <link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
  <link rel="shortcut icon" type="image/x-icon" href="images/misc/personal.png">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Bulma Version 0.7.5-->
  <link rel="stylesheet" href="https://unpkg.com/bulma@0.7.5/css/bulma.min.css" />
  <link href='css/style.css?_t=20200916' rel='stylesheet' type='text/css'>
  <script defer src="font-awesome/js/brands.min.js"></script>
  <script defer src="font-awesome/js/regular.min.js"></script>
  <script defer src="font-awesome/js/fontawesome.min.js"></script>
    <style>
      .red {
        color: rgb(238, 76, 44);
        font-style: normal;
      }
      .blue {
        color: rgb(238, 76, 44);
        font-style: normal;
      }
       
      #intro {
       margin-top: 0em !important;
      }
      
      .content h3 {
       margin-bottom: 1em!important;
       margin-top: 2em!important;
      }

      .content figure {
       width: 90%;
       display: flex;
       align-items: center;
       overflow: hidden;
       margin-left: auto!important;
       margin-right: auto!important;
      }
      
      .columns:not(:last-child) {
       margin-bottom: 1.75rem!important;
      }

      #sidebar {
        width: 75%; 
      }

      @media screen and (min-width: 769px), print {
        .column.is-2_5, .column.is-2-tablet {
          flex: none;
          width: 20%;
        }
      }
</style>
    <script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
</head>


<body>
  <section class="section">
    <div class="container">
      <div class="columns">
        <div class="column is-2_5">
          <div class="sticky">
            <figure class="image" style="width: 11.6rem; margin-top: 6px;">
              <img src="images/misc/homepage3.jpg">
            </figure>
            <div class="content">
              <h2 style="margin-top: 1em">Xuecheng Wu</h2>
              <p>
                01s | Multi-modal Learning<br/> 
                Xi'an City, Shaanxi<br/> 
                Xi'an Jiaotong University<br/>
                Origin: Xinxiang City, Henan<br/>
              </p>
            </div>
            <!-- social network icons -->
<!--             <div class="social">
              <a href="https://github.com/XuecWu" target="_blank">
                <span class="fab fa-github fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a;"></span>
              </a>
              <a href="https://scholar.google.com.hk/citations?user=MuTEp7sAAAAJ" target="_blank">
                <span class="fab fa-google fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
              <a href="mailto:xuecwu@gmail.com" target="_blank">
                <span class="fa-regular fa-envelope fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
            </div> -->
            <div class="social">
              <a href="https://github.com/XuecWu" target="_blank">
                <span class="fab fa-github fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a;"></span>
              </a>
              <a href="https://scholar.google.com.hk/citations?user=MuTEp7sAAAAJ" target="_blank">
                <span class="fab fa-google fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
              <a href="mailto:xuecwu@gmail.com" target="_blank">
                <span class="fa-regular fa-envelope fa-2x" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px"></span>
              </a>
              <a href="javascript:void(0);" title="WeChat" style="display:inline; text-decoration: none; color: #4a4a4a; margin-left: 15px;" onmouseover="showWeChatQRCode()" onmouseout="hideWeChatQRCode()">
                <span class="fab fa-weixin fa-2x"></span>
              </a>
            </div>
            
            <div id="wechat-qr-code" style="display: none; position: absolute; z-index: 100;">
              <img src="images/misc/wechat2.jpg" alt="WeChat QR Code" style="width: 200px; height: 200px;" />
            </div>
            
            <script>
            function showWeChatQRCode() {
              var qrCode = document.getElementById('wechat-qr-code');
              qrCode.style.display = 'block';
            }
            
            function hideWeChatQRCode() {
              var qrCode = document.getElementById('wechat-qr-code');
              qrCode.style.display = 'none';
            }
            </script>
            
            <style>
            .fab.fa-weixin.fa-2x {
              color: #4a4a4a;
            }
            .fab.fa-weixin.fa-2x:hover {
              color: #00ff00;
            }
            </style>


            <div id="sidebar" class="menu sticky is-hidden-mobile">
              <!-- <p class="menu-label"><b>Quick Links</b></p> -->
              <ul class="menu-list">
                <li><a href="#intro">About Me</a></li>
                <li><a href="#news">News</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#publications">Publications</a></li>
<!--                 <li><a href="#awards">Project Experience</a></li> -->
                <li><a href="#Awards">Awards</a></li>
                <li><a href="#Honors">Honors</a></li>
                <li><a href="#Academic Services">Academic Services</a></li>
<!--                 <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=K5mXqzOGNWeXE2Ezi93zbcP2GhxzuJjlVPOeC5nKM24&cl=ffffff&w=a"></script> -->
                <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=8SUH0anSrOSbYt9jGqUOIVDF_nbIFHlEyDrM0-Tyc4E&cl=ffffff&w=a"></script>
<!--                 <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=cB6il4B36H3yx_wuGYRX__86Hp3X7WqnxlyjrsHRIEg&cl=ffffff&w=a"></script> -->
<!--                 <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=cB6il4B36H3yx_wuGYRX__86Hp3X7WqnxlyjrsHRIEg&co=2a5471&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script> -->
              </ul>
            </div>
          </div>



        </div>
        <div class="column right-panel">
          <div class="content">


            <!--About Me-->
            <h2 id="intro">‚ú®About Me</h2>
            <p>
              I'm currently a second-year master student majored in computer technology at the School of Computer Science and Technology, <a href="https://www.xjtu.edu.cn/" target="_blank">Xi'an Jiaotong University</a> (XJTU), supervised by Prof. <a href="https://gr.xjtu.edu.cn/en/web/hlsun" target="_blank">Heli Sun</a>. My research interest lies in deep learning and multi-media computing, primarily focusing on large-scale self-supervised video understanding, multi-modal large langugae models (MLLMs), and misinfo detection.
            </p>
            <p>
              Prior to that, I recevied my B.E. degree at the School of Cyber Science and Engineering, <a href="https://www.zzu.edu.cn/" target="_blank">Zhengzhou University</a>, where I worked closely with Prof. <a href="https://xuejx7.github.io/" target="_blank">Junxiao Xue (PI, Zhejiang Lab)</a> and Prof. <a href="http://softschool.zzu.edu.cn/front/singleArticleDetail?id=4a453ec87076342b017076a5ace80011" target="_blank">Lei Shi (Vice Dean)</a>.
            </p>

            <p>
              My CSDN Technology Blog is located at <a href="https://blog.csdn.net/m0_47623548" target="_blank">HERE</a>.
            </p>

            <p>
              Please feel free to contact me if you are interested in my works and want to explore potential collaborations üôå.
            </p>

            <ul>
              <li><strong>Video Understanding:</strong> multi-modal self-supervised learning</li>
              <li><strong>Multi-modal Large Language Models (MLLMs):</strong> Human-centric, Omini-modal joint modeling, Diffusion Interleaving</li>
              <li><strong>Misinfo Detection:</strong> Deepfake and AIGC detection</li>
            </ul>
          

            <h2 id="news">üëÄNews</h2>
            <ul>
              <div style="width: 1000px;height: 160px; overflow-x:hidden;">
<!--               <div style="width: 1000px;height: 260px; overflow-x:hidden;"> -->
              <li>[12/2024] Two papers are submitted to ICME'25.</li>
              <li>[11/2024] Five papers are submitted to CVPR'25.</li>
              <li>[11/2024] One paper is submitted to Big Data Mining and Analytics.</li>
              <li>[11/2024] Served as a reviewer for CVPR'25.</li>
              <li>[10/2024] Served as a reviewer for WWW'25.</li>
              <li>[09/2024] One paper is submitted to ICASSP'25.</li>
              <li>[08/2024] One paper is submitted to AAAI'25.</li>
              <li>[08/2024] One paper is accpeted by Big Data Mining and Analytics.</li>
              <li>[07/2024] One paper is accpeted by ACM MM24.</li>
              <li>[05/2024] Served as a reviewer for NeurIPS'24.</li>
              <li>[03/2024] Served as a reviewer for MM'24.</li>
              </div>
            </ul>
            
            <!--Experience-->
            <h2 id="experience" style="margin-bottom: 25px;">üòòExperience</h2>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/misc/renminwang.jpg" class="img-logo">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Visiting Stundent</b> | State Key Laboratory of Communication Content Cognition<br>
<!--                     Time: 8/2024 - Present. Advisor:  -->
                    Period: 10/2023 - Present. Advisor: Prof. <a href="https://gr.xjtu.edu.cn/en/web/hlsun" target="_blank">Heli Sun</a>
                  </p>
                </div>
              </div>
            </article>
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/misc/ZZU.jpg" class="img-logo">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Student PI</b> | eMotionAI, Zhengzhou University<br>
                    Period: 6/2021 - 6/2023. Advisor: Prof. <a href="https://xuejx7.github.io/" target="_blank">Junxiao Xue</a>
                  </p>
                </div>
              </div>
            </article>
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/misc/ZZU.jpg" class="img-logo">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Research Assistant</b> | Computational Learning Lab, Zhengzhou University<br>
                    Period: 9/2020 - 6/2023. Supervisor: Prof. <a href="https://xuejx7.github.io/" target="_blank">Junxiao Xue</a>
                  </p>
                </div>
              </div>
            </article>
            

<!--             <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/misc/UST1.jpg" class="img-logo">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Research Intern</b> | CityMind Lab, HKUST(GZ)<br>
                    Time: 4/2023 - 10/2023. Advisor: Prof. <a href="https://scholar.google.com/citations?user=n9cODgcAAAAJ" target="_blank">Yuxuan Liang</a> (<a href="https://www.hkust-gz.edu.cn/academics/hubs-and-thrust-areas/systems-hub/intelligent-transportation/" target="_blank">IT Thrust</a> & <a href="https://www.hkust-gz.edu.cn/academics/hubs-and-thrust-areas/information-hub/data-science-and-analytics/" target="_blank">DSA Thrust</a>)
                  </p>
                </div>
              </div>
            </article> -->

            <!--Publications-->
<!--             <h2 id="publications">
              Publications
              <span style="font-size: 1rem;margin-left: 1rem;position: relative;bottom: .2rem;">
                <a href="https://scholar.google.com/citations?hl=en&user=BGUdPBAAAAAJ" target="_blank" style="font-size: 21px;">
                  [Google Scholar]
                </a>
              </span>
            </h2> -->
            <h2 id="publications" style="margin-bottom: 25px;">üòç Selected Publications</h2>
            

<!--             List of publications-->
<!--             <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/personagen.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Let‚Äôs be different: Personality-driven Motion Generation for Psychologically Diverse LLM Reactions</b><br>
                    <b>Haodong Chen</b>, Yongle Huang, Haojian Huang, Xinxiang Yin, Dian Shao<sup>&dagger;</sup><br>
                    <i>Under Review</i><br>
                    <a href="https://arxiv.org/abs/2405.07472" target="_blank">[Arxiv]</a>
                    <a href="https://haroldchen19.github.io/PersonaGen-Page/" target="_blank">[Project]</a>
                    <a href="https://github.com/HaroldChen19/PersonaGen" target="_blank">[Code]</a><br>
                  </p>
                </div>
              </div>
            </article> -->
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/AVF-MAE++.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>AVF-MAE++: Scaling Affective Video Facial Masked Autoencoders via Efficient Audio-Visual Self-Supervised Learning</b><br>
                    <b>Xuecheng Wu</b>, Heli Sun<sup>&dagger;</sup>, Yifan Wang, Jiayu Nie, Jie Zhang, Yabing Wang, Junxiao Xue, Liang He<br>
<!--                     <i>ACM International World Wide Web Conference (<b>WWW</b>), 2024</i><br> -->
                    <i><b>Under Review, 2025</b></i><br>
<!--                     <a href="https://www.nature.com/articles/s41592-023-01936-6" target="_blank">[Paper]</a> -->
<!--                     <em class="blue"><b>Oral Presentation</b></em>  -->
                  </p>
                </div>
              </div>
            </article>

            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/eMotions.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Towards Emotion Analysis in Short-form Videos: A Large-Scale Dataset and Baseline</b><br>
                    <b>Xuecheng Wu</b>, Heli Sun<sup>&dagger;</sup>, Junxiao Xue, Jiayu Nie, Xiangyan Kong, Ruofan Zhai, Liang He<br>
<!--                     <i>ACM International World Wide Web Conference (<b>WWW</b>), 2024</i><br> -->
                    <i><b>Under Review, 2025</b></i><br>
<!--                     <a href="https://www.nature.com/articles/s41592-023-01936-6" target="_blank">[Paper]</a> -->
<!--                     <em class="blue"><b>Oral Presentation</b></em>  -->
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/MA-YOLO.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>MA-YOLO: Multiple Attention Enhanced Real-time Object Detectors with Triple Discriminative Awareness and Coordinated Representations</b><br>
                    Junxiao Xue*, <b>Xuecheng Wu</b>*<sup>&dagger;</sup>, Liangyu Fu, Jiayu Nie, Jia Zhang, Mengmeng Tian, Shihao Wang, Danlei Huang<br>
<!--                     <i>ACM International World Wide Web Conference (<b>WWW</b>), 2024</i><br> -->
                    <i><b>Under Review, 2025</b></i><br>
<!--                     <a href="https://github.com/HaroldChen19/GaussianVTON" target="_blank">[Code]</a> -->
<!--                     <em class="blue"><b>Oral Presentation</b></em>  -->
                  </p>
                </div>
              </div>
            </article>
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/AV-LGNN.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Building Robust Video-Level Deepfake Detection via Audio-Visual Local-Global Interactions</b><br>
                    Yifan Wang*, <b>Xuecheng Wu</b>*, Jia Zhang, Mohan Jing, Keda Lu, Jun Yu<sup>&dagger;</sup>, Wen Su, Fang Gao, Qingsong Liu, Jianqing Sun, Jiaen Liang<br>
<!--                     <i>ACM International World Wide Web Conference (<b>WWW</b>), 2024</i><br> -->
                    <i><b>ACM International Conference on Multimedia (<b>MM</b>), 2024</b></i><br>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3688985" target="_blank">[Paper]</a>
<!--                     <em class="blue"><b>Oral Presentation</b></em>  -->
                  </p>
                </div>
              </div>
            </article>
            
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/ÁªºËø∞.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Affective Video Content Analysis: Decade Review and New Perspectives</b><br>
                    Junxiao Xue, Jie Wang<sup>&dagger;</sup>, Xiaozhen Liu, Qian Zhang, <b>Xuecheng Wu</b><br>
                    <i><b>Big Data Mining and Analytics</b></i><br>
                  </p>
                </div>
              </div>
            </article>


            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/Masks.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>A Method on Mask Wearing Detection of Natural Population Based on Improved YOLOv4</b><br>
                    Junxiao Xue*, <b>Xuecheng Wu</b>*, Shihao Wang, Mengmeng Tian, Lei Shi<sup>&dagger;</sup><br>
                    <i><b>Journal of Zhengzhou University (Engineering Science)</b></i><br>
                    <a href="https://kns.cnki.net/kcms2/article/abstract?v=lX50VqaRjdV_tWyvzIpstQVUerFqJs0Ah_b8dYINYJ7tBfJ_qpk8gQmnTjL6ZwVuCI8ZZ2YXsup9tp_DZqzbAgtLy_kAJ32b7gYgEZcC0eNnxomtNorPZCggnMpzxWf0bNf8hHTKdeVavh6xO6Haem7Ve62tGpIBxba1nrJIzCC4hYGz3TTgSquw6k_l-O2e&uniplatform=NZKPT&language=CHS" target="_blank">[Paper]</a><br>
                  </p>
                </div>
              </div>
            </article>


            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/ICVNet.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>ICVNet: A Method on Cross-modal Fusion of Short Video Emotion Recognition</b><br>
                    Junxiao Xue*, <b>Xuecheng Wu</b>*, Qian Zhang, Mengmeng Tian, Lanhang Zhai, Lei Shi<sup>&dagger;</sup><br>
                    <i><b>Chinese Journal of Ergonomics</b></i><br>
                    <a href="https://kns.cnki.net/kcms2/article/abstract?v=lX50VqaRjdV_tWyvzIpstQVUerFqJs0Ah_b8dYINYJ4jtUVPsxCB9JG7BTdmyQFRDRZUO8E8cLrOAY-isRT7aUWSFpTNZ0OTIq4C09pLMz6g-CRMsJssXjXkAyUAA2YCnGI_UncN7U_K67-BGU6DaSCIjFBHL4N73MiKAzgVDe0fOk49vrSvY5V1vs2Y0PAt&uniplatform=NZKPT&language=CHS" target="_blank">[Paper]</a><br>
                  </p>
                </div>
              </div>
            </article>


<!--             <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lin2023structure.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform</b><br>
                    <b>Zudi Lin</b>, Donglai Wei, Aarush Gupta, Xingyu Liu, Deqing Sun, and Hanspeter Pfister<br>
                    <i>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2023</i><br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-43898-1_51" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2310.05262" target="_blank">[arXiv]</a>
                    <a href="https://github.com/zudi-lin/pytorch_connectomics/blob/master/connectomics/data/utils/data_transform.py" target="_blank">[Code]</a><br>
                    <em class="blue"><b>Oral Presentation</b></em>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/franco2023current.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Current Progress and Challenges in Large-scale 3D Mitochondria Instance Segmentation</b><br>
                    Daniel Franco-Barranco,  <b>Zudi Lin</b>, Won-Dong Jang, Xueying Wang, Qijia Shen, Wenjie Yin, Yutian Fan,
                    Mingxing Li, Chang Chen, Zhiwei Xiong, Rui Xin, Hao Liu, Huai Chen, Zhili Li, Jie Zhao, Xuejin Chen, Constantin Pape,
                    Ryan Conrad, Jozefus De Folter, Luke Nightingale, Martin Jones, Yanling Liu, Dorsa Ziaei, Stephan Huschauer,
                    Ignacio Arganda-Carreras, Hanspeter Pfister, and Donglai Wei<br>
                    <i>IEEE Transactions on Medical Imaging (<b>TMI</b>), 2023</i><br>
                    <a href="https://ieeexplore.ieee.org/document/10266382" target="_blank">[Paper]</a>
                    <a href="https://www.techrxiv.org/articles/preprint/Current_Progress_and_Challenges_in_Large-scale3D_Mitochondria_Instance_Segmentation/22116785" target="_blank">[TechRxiv]</a>
                    <a href="https://mitoem.grand-challenge.org" target="_blank">[Challenge]</a>
                    <a href="https://github.com/danifranco/TIMISE" target="_blank">[Code]</a><br>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/huang2023domain.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Domain-Scalable Unpaired Image Translation via Latent Space Anchoring</b><br>
                    Siyu Huang, Jie An, Donglai Wei, <b>Zudi Lin</b>, Jiebo Luo, and Hanspeter Pfister<br>
                    <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2023</i><br>
                    <a href="https://ieeexplore.ieee.org/document/10158033" target="_blank">[Paper]</a>
                    <a href="https://github.com/siyuhuang/Latent-Space-Anchoring" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lauenburg2023domain.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>3D Domain Adaptive Instance Segmentation via Cyclic Segmentation GANs</b><br>
                    Leander Lauenburg, <b>Zudi Lin</b>, Ruihan Zhang, M&aacutercia dos Santos, Siyu Huang, Ignacio Arganda-Carreras,
                    Edward S. Boyden, Hanspeter Pfister, and Donglai Wei<br>
                    <i>IEEE Journal of Biomedical and Health Informatics (<b>J-BHI</b>), 2023</i><br>
                    <a href="https://ieeexplore.ieee.org/document/10138543" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2204.03082" target="_blank">[arXiv]</a>
                    <a href="https://connectomics-bazaar.github.io/proj/CySGAN/index.html" target="_blank">[Project Page]</a>
                    <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master/projects/CySGAN" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/zhao2023cddfuse.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion</b><br>
                    Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu,  <b>Zudi Lin</b>, Radu Timofte, and Luc Van Gool<br>
                    <i>Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023</i><br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2211.14461" target="_blank">[arXiv]</a>
                    <a href="https://github.com/Zhaozixiang1228/MMIF-CDDFuse" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lin2023reco.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Relaxing Contrastiveness in Multimodal Representation Learning</b><br>
                    <b>Zudi Lin</b>, Erhan Bas, Kunwar Yashraj Singh, Gurumurthy Swaminathan, and Rahul Bhotika<br>
                    <i>Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2023</i><br>
                    <a href="https://openaccess.thecvf.com/content/WACV2023/html/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.html" target="_blank">[Paper]</a>
                    <a href="https://www.amazon.science/publications/relaxing-contrastiveness-in-multimodal-representation-learning" target="_blank">[Amazon Science]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/cai2022mst++.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction</b><br>
                    Yuanhao Cai, Jing Lin, <b>Zudi Lin</b>, Haoqian Wang, Yulun Zhang, Hanspeter Pfister, Radu Timofte, and Luc Van Gool<br>
                    <i>Computer Vision and Pattern Recognition Workshops (<b>CVPRW</b>), 2022</i><br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Cai_MST_Multi-Stage_Spectral-Wise_Transformer_for_Efficient_Spectral_Reconstruction_CVPRW_2022_paper.pdf" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2204.07908" target="_blank">[arXiv]</a>
                    <a href="https://github.com/caiyuanhao1998/MST-plus-plus" target="_blank">[Code]</a>
                    <a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.pdf" target="_blank">[Challenge Paper]</a><br>
                    <em class="blue"><b>Winner of NTIRE 2022 Challenge on Spectral Reconstruction</b></em>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/magid2022texture.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Texture-based Error Analysis for Image Super-Resolution</b><br>
                    Salma Abdel Magid, <b>Zudi Lin</b>, Donglai Wei, Yulun Zhang, Jinjin Gu and Hanspeter Pfister<br>
                    <i>Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022</i><br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Magid_Texture-Based_Error_Analysis_for_Image_Super-Resolution_CVPR_2022_paper.pdf" target="_blank">[Paper]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/wei2022youmvos.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>YouMVOS: An Actor-centric Multi-shot Video Object Segmentation Dataset</b><br>
                    Donglai Wei, Siddhant Kharbanda, Sarthak Arora, Roshan Roy, Nishant Jain, Akash Palrecha, Tanav
                    Shah, Shray Mathur, Abhijay Kemkar, Ritik Mathur, Anirudh Chakravarthy, <b>Zudi Lin</b>, Won-Dong
                    Jang, Yansong Tang, Song Bai, James Tompkin, Philip Torr and Hanspeter Pfister<br>
                    <i>Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022</i><br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_YouMVOS_An_Actor-Centric_Multi-Shot_Video_Object_Segmentation_Dataset_CVPR_2022_paper.pdf" target="_blank">[Paper]</a>
                    <a href="https://donglaiw.github.io/proj/youMVOS/" target="_blank">[Project Page]</a>
                    <a href="https://drive.google.com/drive/folders/1XBgM-VrFfwOz7p1m5Ts2Vo81O7_qAXKy" target="_blank">[Dataset]</a><br>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/zhao2022discrete.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Discrete Cosine Transform Network for Guided Depth Super-Resolution</b><br>
                    Zixiang Zhao, Jiangshe Zhang, Shuang Xu, <b>Zudi Lin</b> and Hanspeter Pfister<br>
                    <i>Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022</i><br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Discrete_Cosine_Transform_Network_for_Guided_Depth_Map_Super-Resolution_CVPR_2022_paper.pdf" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2104.06977" target="_blank">[arXiv]</a>
                    <a href="https://github.com/Zhaozixiang1228/GDSR-DCTNet" target="_blank">[Code]</a><br>
                    <em class="blue"><b>Oral Presentation</b></em>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/shapson2021connectomics.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>A Connectomic Study of A Petascale Fragment of Human Cerebral Cortex</b><br>
                    Alexander Shapson-Coe, Micha≈Ç Januszewski, Daniel R. Berger, Art Pope, Yuelong Wu, Tim Blakely,
                    Richard L. Schalek, Peter Li, Shuohong Wang, Jeremy Maitin-Shepard, Neha Karlupia, Sven Dorkenwald, 
                    Evelina Sjostedt, Laramie Leavitt, Dongil Lee, Luke Bailey, Angerica Fitzmaurice, Rohin Kar,
                    Benjamin Field, Hank Wu, Julian Wagner-Carena, David Aley, Joanna Lau, <b>Zudi Lin</b>, Donglai Wei,
                    Hanspeter Pfister, Adi Peleg, Viren Jain and Jeff W. Lichtman<br>
                    <i>bioRxiv, 2021</i><br>
                    <a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v2" target="_blank">[bioRxiv]</a>
                    <a href="https://h01-release.storage.googleapis.com/landing.html" target="_blank">[Dataset]</a> 
                    <a href="https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html" target="_blank">[<b>Google AI Blog</b>]</a>
                    <a href="https://www.technologyreview.com/2021/08/25/1031458/scientific-mysteries-human-brain/" target="_blank">[MIT Tech Review]</a>
                    <a href="https://www.scientificamerican.com/article/mapping-the-brain-to-understand-the-mind/" target="_blank">[Scientific American]</a>
                    <!-- <a href="https://www.quantamagazine.org/new-brain-maps-can-predict-behaviors-20211206/" target="_blank">[Quanta Magazine]</a>
                   </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lin2021pytorch.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>PyTorch Connectomics: A Scalable and Flexible Segmentation Framework for EM Connectomics</b><br>
                    <b>Zudi Lin</b>, Donglai Wei, Jeff Lichtman and Hanspeter Pfister<br>
                    <i>arXiv preprint arXiv:2112.05754, 2021</i><br>
                    <a href="https://arxiv.org/abs/2112.05754" target="_blank">[arXiv]</a>
                    <a href="https://github.com/zudi-lin/pytorch_connectomics" target="_blank">[Code]</a>
                    <a href="https://connectomics.readthedocs.io/" target="_blank">[<b>Documentation</b>]</a><br>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/magid2021dynamic.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Dynamic High-Pass Filtering and Multi-Spectral Attention for Image Super-Resolution</b><br>
                    Salma Abdel Magid, Yulun Zhang, Donglai Wei, Won-Dong Jang, <b>Zudi Lin</b>, Yun Fu and Hanspeter Pfister<br>
                    <i>International Conference on Computer Vision (<b>ICCV</b>), 2021</i><br>
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Magid_Dynamic_High-Pass_Filtering_and_Multi-Spectral_Attention_for_Image_Super-Resolution_ICCV_2021_paper.html" target="_blank">[Paper]</a>
                    <a href="https://github.com/sabdelmagid/DFSA_ICCV21" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/yang2021asymmetric.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Asymmetric 3D Context Fusion for Universal Lesion Detection</b><br>
                    Jiancheng Yang, Yi He, Kaiming Kuang, <b>Zudi Lin</b>, Hanspeter Pfister and Bingbing Ni<br>
                    <i>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2021</i><br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-87240-3_55" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2109.08684" target="_blank">[arXiv]</a>
                    <a href="https://github.com/M3DV/AlignShift" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lin2021nucmm.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale</b><br>
                    <b>Zudi Lin</b>, Donglai Wei, Mariela D. Petkova, Yuelong Wu, Zergham Ahmed, Krishna Swaroop K, Silin
                    Zou, Nils Wendt, Jonathan Boulanger-Weill, Xueying Wang, Nagaraju Dhanyasi, Ignacio Arganda-Carreras, 
                    Florian Engert, Jeff Lichtman and Hanspeter Pfister<br>
                    <i>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2021</i><br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-87193-2_16" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2107.05840" target="_blank">[arXiv]</a>
                    <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master/configs/NucMM" target="_blank">[Code]</a>
                    <a href="https://connectomics-bazaar.github.io/proj/nucMM/index.html" target="_blank">[Project Page]</a><br>
                    <em class="blue"><b>Student Travel Award</b></em>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/wei2021axonem.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions</b><br>
                      Donglai Wei, Kisuk Lee, Hanyu Li, Ran Lu, J. Alexander Bae, Zequan Liu, Lifu Zhang, M√°rcia
                      dos Santos, <b>Zudi Lin</b>, Thomas Uram, Xueying Wang, Ignacio Arganda-Carreras, Brian Matejek,
                      Narayanan Kasthuri, Jeff Lichtman and Hanspeter Pfister<br>
                    <i>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2021</i><br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-87193-2_17" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/2107.05451" target="_blank">[arXiv]</a>
                    <a href="https://github.com/donglaiw/AxonEM-challenge" target="_blank">[Code]</a>
                    <a href="https://connectomics-bazaar.github.io/proj/AxonEM/index.html" target="_blank">[Project Page]</a><br>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/rademacher2021wood.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>The Wood Image Analysis and Dataset (WIAD): open-access visual analysis tools to advance the ecological data revolution</b><br>
                    Tim Rademacher, Bijan Seyednasrollah, David Basler, Jian Cheng, Tessa Mandra, Elise Miller, <b>Zudi Lin</b>, 
                    David A Orwig, Neil Pederson, Hanspeter Pfister, Andrew D Richardson, Donglai Wei and Li Yao<br>
                    <i><b>Methods in Ecology and Evolution</b>, 2021</i><br>
                    <a href="https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13717" target="_blank">[Paper]</a>
                    <a href="https://www.biorxiv.org/content/10.1101/2020.12.16.423133v2" target="_blank">[bioRxiv]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lin2020two.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>Two-Stream Active Query Suggestion for Active Learning in Connectomics</b><br>
                    <b>Zudi Lin</b>, Donglai Wei, Won-Dong Jang, Siyan Zhou, Xupeng Chen, Xueying Wang, Richard Schalek, Daniel Berger, Adi Suissa-Peleg, 
                    Brian Matejek, Lee Kamentsky, Toufiq Parag, Thouis Jones, Daniel Haehn, Jeff Lichtman and Hanspeter Pfister<br>
                    <i>European Conference on Computer Vision (<b>ECCV</b>), 2020</i><br>
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630103.pdf" target="_blank">[Paper]</a>
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630103-supp.pdf" target="_blank">[Supp.]</a>
                    <a href="https://github.com/zudi-lin/pytorch_connectomics/blob/master/scripts/tools/two_stream.py" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/wei2020mitoem.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>MitoEM Dataset: Large-scale 3D Mitochondria Instance Segmentation from EM Images</b><br>
                    Donglai Wei, <b>Zudi Lin</b>, Daniel Barranco, Nils Wendt, Xingyu Liu, Wenjie Yin, Xin Huang, Aarush Gupta, Won-Dong Jang, 
                    Xueying Wang, Ignacio Arganda-Carreras, Jeff Lichtman, Hanspeter Pfister<br>
                    <i>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2020</i><br>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-59722-1_7" target="_blank">[Paper]</a>
                    <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master/configs/MitoEM" target="_blank">[Code]</a>
                    <a href="https://mitoem.grand-challenge.org/" target="_blank">[<b>Challenge</b>]</a>
                    <a href="https://connectomics.readthedocs.io/en/latest/tutorials/mito.html" target="_blank">[Tutorial]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/talwar2020topological.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>A Topological Nomenclature for 3D Shape Analysis in Connectomics</b><br>
                    Abhimanyu Talwar, <b>Zudi Lin</b>, Donglai Wei, Yuesong Wu, Bowen Zheng, Jinglin Zhao, 
                    Won-Dong Jang, Xueying Wang, Jeff Lichtman, and Hanspeter Pfister<br>
                    <i>Computer Vision and Pattern Recognition Workshops (<b>CVPRW</b>), 2020</i><br>
                    <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w57/Talwar_A_Topological_Nomenclature_for_3D_Shape_Analysis_in_Connectomics_CVPRW_2020_paper.pdf" target="_blank">[Paper]</a>
                    <a href="https://github.com/donglaiw/ibexHelper" target="_blank">[Code]</a><br>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/lin2019white.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>White-Box Adversarial Defense via Self-Supervised Data Estimation</b><br>
                    <b>Zudi Lin</b>, Hanspeter Pfister and Ziming Zhang<br>
                    <i>arXiv preprint arXiv:1909.06271, 2019</i><br>
                    <a href="https://arxiv.org/abs/1909.06271" target="_blank">[arXiv]</a>
                    <a href="https://github.com/zudi-lin/RIDE" target="_blank">[Code]</a>
                  </p>
                </div>
              </div>
            </article>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/publications/dennig2019fdive.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <b>FDive: Learning Relevance Models using Pattern-based Similarity Measures</b><br>
                    Frederik Dennig, Tom Polk, <b>Zudi Lin</b>, Tobias Schreck, Hanspeter Pfister, and Michael Behrisch<br>
                    <i>IEEE Conference on Visual Analytics Science and Technology (<b>VAST</b>), 2019</i><br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/8986940" target="_blank">[Paper]</a>
                    <a href="https://arxiv.org/abs/1907.12489" target="_blank">[arXiv]</a>
                  </p>
                </div>
              </div>
            </article> -->
            
            <!--Awards-->
<!--             <h2 id="awards">Project Experience</h2>
            <ul>
              <li><b>Fine-Grained Action Recognition</b> (9/2023-11/2023)
                <ul>
                <li>The first semi‚Äësupervised fine‚Äëgrained action recognition framework based on transformer;</li>
                <li>Proposed multi-granularity random sampling with a simple but effective temporal data augmentation method to better learn inter-frame fine-grained action information;</li>
                <li>Designed a threshold adaptive approach to effectively mitigate the problem of uneven confidence in video data and time consumption during debugging.</li>
                </ul>
              </li>
              <li><b>Contrastive Urban Region Profiling</b> (7/2023-10/2023)
                <ul>
                <li>The first‚Äëever framework that integrates the knowledge of text modality into urban region profiling;</li>
                <li>Blending text knowledge into visual representations through deep modality interaction, utilizing contrastive and language modeling losses;</li>
                <li>Developed a novel web‚Äëbased application enabled by the proposed model to offer insights about urban planning.</li>
                </ul>
              </li>
              <li><b>Parent-Child Dialogue Language Model</b> (7/2023-9/2023)
                <ul>
                <li>Leveraging the paradigm shift of Large Language Models for a new form of human‚Äëcomputer interaction;</li>
                <li>Aiming to enhance familial harmony by establishing an advanced framework for reshaping the family landscape.</li>
                </ul>
              </li>
            </ul> -->

            <h2 id="Awards">üéâAwards</h2>
            <ul>
<!--               <li>Research and Innovation Scholarship of NPU (Top 3&lsquo;), 2024.</li> -->
              <li>2024Âπ¥Á¨¨ÂçÅ‰πùÂ±ä"ÊåëÊàòÊùØ"ÂÖ®ÂõΩÂ§ßÂ≠¶ÁîüËØæÂ§ñÂ≠¶ÊúØÁßëÊäÄ‰ΩúÂìÅÁ´ûËµõ"Êè≠Ê¶úÊåÇÂ∏Ö"‰∏ìÈ°πËµõ <strong>ÂÖ®ÂõΩ‰∏ÄÁ≠âÂ•ñ</strong></li>
              <li>2024Âπ¥"‰∏≠ÂõΩÁΩëË∞∑¬∑Âçé‰∏∫ÊùØ"‰∏≠ÂõΩÁ†îÁ©∂ÁîüÁΩëÁªúÂÆâÂÖ®ÂàõÊñ∞Â§ßËµõ <strong>ÂÖ®ÂõΩ‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2024Âπ¥ACM MultimediaÂ§ß‰ºö Deepfakes1MÂÖ®ÁêÉÊåëÊàòËµõ <strong>ÂÜ†ÂÜõ</strong></li>
              <li>2024Âπ¥"Âçé‰∏∫ÊùØ"Á¨¨ÂÖ≠Â±ä‰∏≠ÂõΩÁ†îÁ©∂Áîü‰∫∫Â∑•Êô∫ËÉΩÂàõÊñ∞Â§ßËµõ <strong>ÂÖ®ÂõΩ‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2024Âπ¥Á¨¨ÂçÅ‰∫îÂ±ä‰∏≠ÂõΩÂ§ßÂ≠¶ÁîüÊúçÂä°Â§ñÂåÖÂàõÊñ∞Âàõ‰∏öÂ§ßËµõ <strong>ÁúÅÁ∫ß‰∫åÁ≠âÂ•ñ &amp; ÂÖ®ÂõΩ‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2024Âπ¥‰∏≠ÂõΩÈ´òÊ†°ËÆ°ÁÆóÊú∫Â§ßËµõ-ÁΩëÁªúÊäÄÊúØÊåëÊàòËµõ <strong>ÁúÅÁ∫ß‰∫åÁ≠âÂ•ñ</strong></li>
              <li>2022Âπ¥Á¨¨ÂçÅ‰∫îÂ±äÂÖ®ÂõΩÂ§ßÂ≠¶Áîü‰ø°ÊÅØÂÆâÂÖ®Á´ûËµõ-‰ΩúÂìÅËµõ <strong>ÂÖ®ÂõΩ‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2022Âπ¥ÂÖ®ÂõΩÂ§ßÂ≠¶ÁîüÁâ©ËÅîÁΩëËÆæËÆ°Á´ûËµõ <strong>ÁúÅÁ∫ß‰∏ÄÁ≠âÂ•ñ &amp; ÂÖ®ÂõΩ‰∫åÁ≠âÂ•ñ</strong></li>
              <li>2022Âπ¥‰∏≠ÂõΩÈ´òÊ†°ËÆ°ÁÆóÊú∫Â§ßËµõ-ÁΩëÁªúÊäÄÊúØÊåëÊàòËµõ <strong>ÁúÅÁ∫ß‰∫åÁ≠âÂ•ñ &amp; ÂÖ®ÂõΩ‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2022Âπ¥Â§ßÂ≠¶ÁîüÂàõÊñ∞Âàõ‰∏öËÆ≠ÁªÉËÆ°Âàí <strong>Ê≤≥ÂçóÁúÅÊïôËÇ≤ÂéÖÊö®ÈÉëÂ∑ûÂ§ßÂ≠¶ÂàõÊñ∞ÈáçÁÇπÈ°πÁõÆ (È°πÁõÆ‰∏ªÊåÅ‰∫∫)</strong></li>
              <li>2022Âπ¥‰∏≠ÂõΩÂ§ßÂ≠¶ÁîüËÆ°ÁÆóÊú∫ËÆæËÆ°Â§ßËµõ <strong>ÁúÅÁ∫ß‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2021Âπ¥ÂÖ®ÂõΩÂ§ßÂ≠¶ÁîüÁâ©ËÅîÁΩëËÆæËÆ°Á´ûËµõ <strong>ÁúÅÁ∫ß‰∏ÄÁ≠âÂ•ñ &amp; ÂÖ®ÂõΩ‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2021Âπ¥‰∏≠ÂõΩÈ´òÊ†°ËÆ°ÁÆóÊú∫Â§ßËµõ-ÁΩëÁªúÊäÄÊúØÊåëÊàòËµõ <strong>ÁúÅÁ∫ß‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2021Âπ¥‰∏≠ÂõΩÂ§ßÂ≠¶ÁîüËÆ°ÁÆóÊú∫ËÆæËÆ°Â§ßËµõ <strong>ÁúÅÁ∫ß‰∏âÁ≠âÂ•ñ</strong></li>
              <li>2021Âπ¥Á¨¨ÂçÅ‰∏ÉÂ±ä "ÊåëÊàòÊùØ"Â§ßÂ≠¶ÁîüËØæÂ§ñÂ≠¶ÊúØÁßëÊäÄ‰ΩúÂìÅÁ´ûËµõ <strong>ÈÉëÂ∑ûÂ§ßÂ≠¶Ê†°Á∫ß‰∫åÁ≠âÂ•ñ</strong></li>
              <li>2020Âπ¥ÂÖ®ÂõΩÂ§ßÂ≠¶ÁîüËã±ËØ≠Á´ûËµõ (NECCS) <strong>ÁúÅÁ∫ß‰ºòÁßÄÂ•ñ</strong></li>
            </ul>


            <h2 id="Honors">üíñHonors</h2>
            <ul>
<!--               <li>Research and Innovation Scholarship of NPU (Top 3&lsquo;), 2024.</li> -->
              <li>Ë•øÂÆâ‰∫§ÈÄöÂ§ßÂ≠¶2023-2024Â≠¶Âπ¥Á†îÁ©∂ÁîüÁâπÁ≠âÂ•ñÂ≠¶Èáë</li>
              <li>Ë•øÂÆâ‰∫§ÈÄöÂ§ßÂ≠¶2023-2024Â≠¶Âπ¥‰ºòÁßÄÁ†îÁ©∂Áîü</li>
              <li>‰∏≠ÂõΩÂ§ßÂ≠¶ÁîüËá™Âº∫‰πãÊòü (<strong>ËØ•Âπ¥Â∫¶ÈÉëÂ∑ûÂ§ßÂ≠¶ÂîØ‰∏ÄÂÖ•ÈÄâ; Ê≤≥ÂçóÁúÅÊéíÂêçÁ¨¨‰∏Ä</strong>) <a href="https://www.zzu.edu.cn/info/1217/81980.htm" target="_blank">(Link)</a></li>
              <li>ÈÉëÂ∑ûÂ§ßÂ≠¶2023Â±äÊôÆÈÄöÊú¨ÁßëÁîü‰ºòÁßÄÊØï‰∏öËÆ∫Êñá (<strong>TOP 1.4%</strong>)</li>
              <li>2023Âπ¥Â∫¶ÈÉëÂ∑ûÂ§ßÂ≠¶‰ºòÁßÄÊØï‰∏öÁîü</li>
              <li>2023Âπ¥Â∫¶ÈÉëÂ∑ûÂ§ßÂ≠¶ÁΩëÁªúÁ©∫Èó¥ÂÆâÂÖ®Â≠¶Èô¢ÂçÅ‰Ω≥‰ºòÁßÄÊØï‰∏öÁîü</li>
              <li>2022Ëá≥2023Â≠¶Âπ¥ÈÉëÂ∑ûÂ§ßÂ≠¶‰∏ÄÁ≠âÂ≠¶‰∏öÂ•ñÂ≠¶Èáë</li>
              <li>2022Âπ¥Â∫¶ÈÉëÂ∑ûÂ§ßÂ≠¶<strong>Ê†°Âõ≠‰πãÊòü</strong></li>
              <li>ÈÉëÂ∑ûÂ§ßÂ≠¶2022-2023Â≠¶Âπ¥"ËúúÈõ™ÂÜ∞Âüé"Â•ñÂ≠¶Èáë</li>
              <li>ÈÉëÂ∑ûÂ§ßÂ≠¶2021Ëá≥2022Â≠¶Âπ¥‰∏âÂ•ΩÂ≠¶Áîü</li>
              <li>ÈÉëÂ∑ûÂ§ßÂ≠¶2021Ëá≥2022Â≠¶Âπ¥‰ºòÁßÄÂ≠¶ÁîüÂπ≤ÈÉ®</li>
              <li>2022Âπ¥Â∫¶ÊïôËÇ≤ÈÉ®-Âçé‰∏∫"Êô∫ËÉΩÂü∫Â∫ß"Â•ñÂ≠¶Èáë</li>
              <li>2021Ëá≥2022Â≠¶Âπ¥ÂõΩÂÆ∂Âä±ÂøóÂ•ñÂ≠¶Èáë</li>
              <li>2021Âπ¥Â∫¶Ê≤≥ÂçóÁúÅÈ´òÊ†°ÊñáÊòéÂÆøËàç (<strong>TOP 0.2%</strong>, ÂÆøËàçÈïø)</li>
              <li>2020Ëá≥2021Â≠¶Âπ¥ÂõΩÂÆ∂Âä±ÂøóÂ•ñÂ≠¶Èáë</li>
              <li>ÈÉëÂ∑ûÂ§ßÂ≠¶2020Ëá≥2021Â≠¶Âπ¥‰∏âÂ•ΩÂ≠¶Áîü</li>
            </ul>


            <h2 id="Academic Services">üåπAcademic Services</h2>
            <ul>
              <li>Conference Reviewer for 
                <br>&emsp;1. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
                <br>&emsp;2. Neural Information Processing Systems (NeurIPS)
                <br>&emsp;3. ACM The Web Conference (WWW)
                <br>&emsp;4. CAAI International Conference on Artificial Intelligence (CICAI)
                <br>&emsp;5. ACM International Conference on Multimedia (MM)
                <br>&emsp;6. IEEE BigData 
              </li>
              <li>Journal Reviewer for 
                <br>&emsp;1. IEEE Transactions on Multimedia (TMM)
                <br>&emsp;2. Knowledge-based Systems (KBS)
                <br>&emsp;3. IEEE Transactions on Knowledge and Data Engineering (TKDE)
              </li>
            </ul>
            
          </div>
        </div>
      </div>
    </div>

    <!-- Footer -->
    <footer class="container">
      <br><hr>
      <div class="row" style="text-align: center">
        ¬© 2024 Xuecheng Wu (Conna)
      </div>
    </footer>
  </section>


  <script>

    var $hashList = $('.menu-list a'), offsetList, maxScrollHeight;

    $('#sidebar').on('click', 'a', function(){
      activate($(this))
    });
    
    $(window).on('resize', debounce(calculateBoundary, 300));

    $(document).on('scroll', debounce(judgeScroll, 300));

    calculateBoundary();
    judgeScroll();
    
    function  calculateBoundary() {
      offsetList = $hashList.map(function(idx, ele){
        return $(ele.hash).offset().top
      });
      maxScrollHeight = $(document).height() - $(window).height()
    }
    
    function judgeScroll() {
      var tps = $("html").scrollTop()
              ? $("html").scrollTop()
              : $("body").scrollTop(),
              len = offsetList.length;
      if(tps >= maxScrollHeight-10){
        activate($hashList.eq(len-1));
        return
      }
      for(var i=0; i<len; i++){
       if(tps+50<offsetList[i]){
          activate($hashList.eq(Math.max(0,i-1)));
          return
        }
      }
    }

    function activate(ele){
      $hashList.removeClass('is-active');
      ele.addClass('is-active');
    }

    function debounce(fn, delay) {
      var timeout = null;
      return function () {
        var args = arguments;
        var context = this;
        if (!timeout) {
          timeout = setTimeout(function () {
            timeout = 0;
            return fn.apply(context, args);
          }, delay);
        }
      };
    }



  </script>


</body>

</html>
